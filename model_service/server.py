import grpc
from concurrent import futures
import time
import os
import sys
import numpy as np

# Add current directory to path so generated modules can be found
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# These will be generated by the build process
try:
    import inference_pb2
    import inference_pb2_grpc
except ImportError:
    print("Error: Protobuf files not found. Did you run code generation?")
    # We don't exit here so the file can be saved even if dependencies aren't ready
    pass

class InferenceService(inference_pb2_grpc.ModelInferenceServicer):
    def Predict(self, request, context):
        model_name = request.model_name or "unknown"
        prompt = request.prompt
        
        print(f"[Inference] üß† Request: Model={model_name}")
        
        start_time = time.time()
        
        # ---------------------------------------------------------
        # LOCAL INFERENCE: Rule-based Sentiment Analysis (Mock)
        # ---------------------------------------------------------
        if prompt:
            print(f"[Inference] üîç Analyzing Text Locally: '{prompt}'")
            time.sleep(0.2) # Simulate compute time
            
            prompt_lower = prompt.lower()
            positive_words = ['good', 'happy', 'love', 'amazing', 'best', 'great', 'awesome']
            negative_words = ['bad', 'sad', 'hate', 'terrible', 'worst', 'poor', 'disappointed']
            
            if any(word in prompt_lower for word in positive_words):
                class_id = 1 # Positive
                confidence = float(np.random.uniform(0.90, 0.99))
            elif any(word in prompt_lower for word in negative_words):
                class_id = 0 # Negative
                confidence = float(np.random.uniform(0.85, 0.95))
            else:
                class_id = 2 # Neutral
                confidence = float(np.random.uniform(0.70, 0.85))
                
            print(f"[Inference] Result: Class={class_id}, Confidence={confidence:.4f}")

        else:
            # ---------------------------------------------------------
            # MOCK COMPUTE (Fallback for Numeric Features)
            # ---------------------------------------------------------
            print(f"[Inference] üé≤ Using Mock Inference (Numeric)")
            time.sleep(0.5) # 500ms Latency Simulation
            
            class_id = int(np.random.randint(0, 3))
            confidence = float(np.random.uniform(0.85, 0.99))
        
        elapsed = time.time() - start_time
        print(f"[Inference] ‚úÖ Processed in {elapsed:.4f}s")
        
        return inference_pb2.PredictResponse(
            class_id=class_id,
            confidence=confidence,
            error=""
        )

def serve():
    print("[Inference] Starting gRPC Server...")
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    inference_pb2_grpc.add_ModelInferenceServicer_to_server(InferenceService(), server)
    
    port = os.environ.get("PORT", "50051")
    server.add_insecure_port(f'[::]:{port}')
    print(f"[Inference] üöÄ Server listening on port {port}")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    serve()
